{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tarfile\n",
        "from shutil import copyfile\n",
        "\n",
        "\n",
        "# Fonction pour créer un répertoire s'il n'existe pas déjà\n",
        "def make_dir(file_path):\n",
        "    if not os.path.exists(file_path):\n",
        "        os.makedirs(file_path)\n",
        "\n",
        "# Fonction pour séparer les images de CUB200 en jeu d'entraînement et de test\n",
        "def separate_train_test(dataset_path, train_path, test_path):\n",
        "    class_index = 1\n",
        "    for classname in sorted(os.listdir(dataset_path)):\n",
        "        if classname.startswith('.'):\n",
        "            continue\n",
        "        # Créer des répertoires pour l'ensemble d'entraînement et de test\n",
        "        make_dir(os.path.join(train_path, classname))\n",
        "        make_dir(os.path.join(test_path, classname))\n",
        "        i = 0\n",
        "        for file in sorted(os.listdir(os.path.join(dataset_path, classname))):\n",
        "            if file.startswith('.'):\n",
        "                continue\n",
        "            file_path = os.path.join(dataset_path, classname, file)\n",
        "            if i < 15:\n",
        "                # Copier les premiers 15 fichiers dans le répertoire de test\n",
        "                copyfile(file_path, os.path.join(test_path, classname, file))\n",
        "            else:\n",
        "                # Copier les autres fichiers dans le répertoire d'entraînement\n",
        "                copyfile(file_path, os.path.join(train_path, classname, file))\n",
        "            i += 1\n",
        "\n",
        "        class_index += 1\n",
        "\n",
        "# Chemin du fichier tarball (TGZ) à extraire\n",
        "tgz_file_path = '/content/CUB_200_2011.tar'\n",
        "\n",
        "# Chemin du répertoire où les fichiers seront extraits\n",
        "extraction_directory = '/content/CUB_200_2011/'\n",
        "\n",
        "# Chemins pour les ensembles d'entraînement et de test\n",
        "train_path = '/content/train/'\n",
        "test_path = '/content/test/'\n",
        "\n",
        "# Créez le répertoire d'extraction s'il n'existe pas\n",
        "make_dir(extraction_directory)\n",
        "\n",
        "!tar -xf /content/CUB_200_2011.tar -C /content/\n",
        "\n",
        "# Chemin vers le dossier images dans le répertoire extrait\n",
        "images_path = os.path.join(extraction_directory, 'images/')\n",
        "\n",
        "# Séparez les données en ensembles d'entraînement et de test\n",
        "separate_train_test(images_path, train_path, test_path)\n",
        "print(f'Les données ont été séparées en ensembles d\\'entraînement dans : {train_path} et de test dans : {test_path}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boMR3yF7wELv",
        "outputId": "9498e8cf-8e01-40d4-8142-c28655f6a004"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Les données ont été séparées en ensembles d'entraînement dans : /content/train/ et de test dans : /content/test/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_asU0RpHptw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.models import resnet18\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Définir la transformation des données pour l'entraînement et le test\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0c3RX71hIe65"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Load training data using ImageFolder with custom filter\n",
        "train_data = ImageFolder(root=train_path, transform=transform_train)\n",
        "\n",
        "# Load testing data using ImageFolder with custom filter\n",
        "test_data = ImageFolder(root=test_path, transform=transform_test)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLfH5k7nx5ZK"
      },
      "source": [
        "Transformation avec Normalisation à partir des données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCQWw5kgsrWi",
        "outputId": "bd4fd7cf-7faf-4ded-ea21-0aad5df80319"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean (train): tensor([0.0110, 0.2021, 0.1252])\n",
            "Std (train): tensor([0.9894, 0.9892, 1.1602])\n"
          ]
        }
      ],
      "source": [
        "# Fonction pour calculer la moyenne et l'écart type d'un dataset\n",
        "def calculate_mean_std(loader):\n",
        "    mean = torch.zeros(3)\n",
        "    std = torch.zeros(3)\n",
        "    num_samples = 0\n",
        "\n",
        "    # Calculer les sommes des valeurs de pixels et des carrés des valeurs de pixels pour chaque canal\n",
        "    for images, _ in loader:\n",
        "        mean += torch.sum(images, dim=[0, 2, 3])\n",
        "        std += torch.sum(images**2, dim=[0, 2, 3])\n",
        "        num_samples += images.size(0)\n",
        "\n",
        "    # Diviser par le nombre total d'échantillons et de pixels par canal\n",
        "    mean /= (num_samples * images.size(2) * images.size(3))\n",
        "    std = torch.sqrt(std / (num_samples * images.size(2) * images.size(3)) - mean**2)\n",
        "\n",
        "    return mean, std\n",
        "\n",
        "# Calculer la moyenne et l'écart type pour l'ensemble d'entraînement\n",
        "train_mean, train_std = calculate_mean_std(train_loader)\n",
        "print(f'Mean (train): {train_mean}')\n",
        "print(f'Std (train): {train_std}')\n",
        "\n",
        "\n",
        "\n",
        "# Vous pouvez maintenant utiliser ces valeurs dans vos transformations pour normaliser les données\n",
        "transform_train_norm = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=train_mean, std=train_std)\n",
        "])\n",
        "\n",
        "transform_test_norm = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=train_mean, std=train_std)\n",
        "])\n",
        "# Load the training and test data\n",
        "train_data_norm = ImageFolder(train_path, transform=transform_train_norm)\n",
        "test_data_norm = ImageFolder(test_path, transform=transform_test_norm)\n",
        "\n",
        "# Define the data loaders for training and testing\n",
        "train_loader_norm = DataLoader(train_data_norm, batch_size=64, shuffle=True)\n",
        "test_loader_norm = DataLoader(test_data_norm, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQplUhCsYK0B"
      },
      "source": [
        "Initialisation aléatoire par défaut:première normalisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJ3eBCmOYM8F",
        "outputId": "7b94c9e0-82e8-4d26-f894-da58ba4ad55b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 - Loss: 5.1437\n",
            "Epoch 2/5 - Loss: 4.7047\n",
            "Epoch 3/5 - Loss: 4.3489\n",
            "Epoch 4/5 - Loss: 4.0344\n",
            "Epoch 5/5 - Loss: 3.7745\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.models import resnet18\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "model = models.resnet18(pretrained=False)\n",
        "num_classes = len(train_data_norm.classes)\n",
        "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader_norm:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "    epoch_loss = running_loss / len(train_data)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(loader, model):\n",
        "    model.eval()\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            try:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                correct_predictions += (predicted == labels).sum().item()\n",
        "                total_samples += labels.size(0)\n",
        "            except OSError:\n",
        "                # Ignore truncated images and continue\n",
        "                continue\n",
        "    accuracy = correct_predictions / total_samples * 100\n",
        "    return accuracy\n",
        "train_accuracy = calculate_accuracy(train_loader_norm, model)\n",
        "print(train_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0angQNL5guz",
        "outputId": "4f7780e2-0214-4854-e17a-8c10a969f314"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15.395994538006372\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vZwdArwZFqM"
      },
      "source": [
        "initialisation aléatoire par défaut : deuxième Normalisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ast_kIk4bqf9",
        "outputId": "b37f9b60-0aab-4c1a-eb03-86f46261374a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 - Loss: 5.1531\n",
            "Epoch 2/5 - Loss: 4.7466\n",
            "Epoch 3/5 - Loss: 4.3243\n",
            "Epoch 4/5 - Loss: 4.0076\n",
            "Epoch 5/5 - Loss: 3.7081\n",
            "12.152935821574875\n"
          ]
        }
      ],
      "source": [
        "model = models.resnet18(pretrained=False)\n",
        "num_classes = len(train_data.classes)\n",
        "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 5  # Set the number of epochs you want to train\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "    epoch_loss = running_loss / len(train_data)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f}\")\n",
        "train_accuracy= calculate_accuracy(train_loader, model)\n",
        "print(train_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2E-H9hlLxhHK"
      },
      "source": [
        "le modèle pré-entraîné, mais en gelant («freeze») tous les paramètres de convolution(La deuxieme methode de normalisation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UREqTCxNd8-R",
        "outputId": "554a61ad-edf3-4b4f-efd3-ed01b6f66ae5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 - Loss: 4.3258\n",
            "Epoch 2/5 - Loss: 2.6652\n",
            "Epoch 3/5 - Loss: 1.9893\n",
            "Epoch 4/5 - Loss: 1.6200\n",
            "Epoch 5/5 - Loss: 1.3840\n",
            "Training Time: 282.187495470047 seconds\n",
            "75.59171597633136\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.models import resnet18\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define the data transformation for training and testing\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load the training and test data\n",
        "train_data = ImageFolder(train_path, transform=transform_train)\n",
        "test_data = ImageFolder(test_path, transform=transform_test)\n",
        "\n",
        "# Define the data loaders for training and testing\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
        "\n",
        "# Load the pre-trained ResNet18 model\n",
        "model = resnet18(pretrained=True)\n",
        "\n",
        "# Freeze all convolutional parameters except the final fully connected layer\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "model.fc.requires_grad = True  # Unfreeze the final fully connected layer\n",
        "\n",
        "# Replace the final fully connected layer for the number of classes in the dataset\n",
        "num_classes = len(train_data.classes)\n",
        "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Train the model\n",
        "start_time = time.time()\n",
        "for epoch in range(5):  # 5 epochs for demonstration\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "    epoch_loss = running_loss / len(train_data)\n",
        "    print(f\"Epoch {epoch+1}/{5} - Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Training Time: {end_time - start_time} seconds\")\n",
        "train_accuracy = calculate_accuracy(train_loader, model)\n",
        "print(train_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GcCKqWMy2HM"
      },
      "source": [
        "le modèle pré-entraîné, mais en gelant («freeze») tous les paramètres de convolution (normalisation à partir des données)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYpzNxbKvk13",
        "outputId": "ff32cb83-55de-4fd2-bb52-26733f732cba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 - Loss: 4.3500\n",
            "Epoch 2/5 - Loss: 2.7004\n",
            "Epoch 3/5 - Loss: 2.0292\n",
            "Epoch 4/5 - Loss: 1.6522\n",
            "Epoch 5/5 - Loss: 1.4248\n",
            "Training Time: 281.53787660598755 seconds\n",
            "75.97860719162495\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Load the training and test data\n",
        "train_data_norm = ImageFolder(train_path, transform=transform_train_norm)\n",
        "test_data_norm = ImageFolder(test_path, transform=transform_test_norm)\n",
        "\n",
        "# Define the data loaders for training and testing\n",
        "train_loader_norm = DataLoader(train_data_norm, batch_size=64, shuffle=True)\n",
        "test_loader_norm = DataLoader(test_data_norm, batch_size=64, shuffle=False)\n",
        "\n",
        "# Load the pre-trained ResNet18 model\n",
        "model = resnet18(pretrained=True)\n",
        "\n",
        "# Freeze all convolutional parameters except the final fully connected layer\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "model.fc.requires_grad = True  # Unfreeze the final fully connected layer\n",
        "\n",
        "# Replace the final fully connected layer for the number of classes in the dataset\n",
        "num_classes = len(train_data_norm.classes)\n",
        "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Train the model\n",
        "start_time = time.time()\n",
        "for epoch in range(5):  # 5 epochs for demonstration\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader_norm:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "    epoch_loss = running_loss / len(train_data_norm)\n",
        "    print(f\"Epoch {epoch+1}/{5} - Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Training Time: {end_time - start_time} seconds\")\n",
        "train_accuracy = calculate_accuracy(train_loader_norm, model)\n",
        "print(train_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJvAkCHq-bPC"
      },
      "source": [
        "le modèle pré-entraîné, mais en gelant uniquement les paramètres dans \"layer1\"avec la deuxieme de normalisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIlyDoCAf2h1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a72880b3-1f93-4a80-f4ab-d845baf3f81d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 - Loss: 3.3223\n",
            "Epoch 2/5 - Loss: 1.7341\n",
            "Epoch 3/5 - Loss: 1.1669\n",
            "Epoch 4/5 - Loss: 0.8319\n",
            "Epoch 5/5 - Loss: 0.5961\n",
            "Training Time: 341.91364789009094 seconds\n",
            "82.0323167956304\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Charger le modèle pré-entraîné ResNet18\n",
        "model = resnet18(pretrained=True)\n",
        "\n",
        "# Geler les paramètres de toutes les couches de convolution\n",
        "for name, param in model.named_parameters():\n",
        "    if 'layer1' in name:\n",
        "        param.requires_grad = False\n",
        "\n",
        "# Remplacer la dernière couche entièrement connectée pour la classification par une nouvelle couche adaptée à notre nombre de classes\n",
        "num_classes = len(train_data.classes)\n",
        "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "\n",
        "# Définir la fonction de perte et l'optimiseur\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Entraînement du modèle\n",
        "start_time = time.time()\n",
        "for epoch in range(5):  # 5 epochs pour l'exemple\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "    epoch_loss = running_loss / len(train_data)\n",
        "    print(f\"Epoch {epoch+1}/{5} - Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Training Time: {end_time - start_time} seconds\")\n",
        "train_accuracy = calculate_accuracy(train_loader, model)\n",
        "print(train_accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xtn222aUL5Gl"
      },
      "source": [
        "le modèle pré-entraîné, mais en gelant uniquement les paramètres dans \"layer1\"avec la premiere  normalisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3D0adkjDuO8"
      },
      "outputs": [],
      "source": [
        "# Charger le modèle pré-entraîné ResNet18\n",
        "model = resnet18(pretrained=True)\n",
        "\n",
        "# Geler les paramètres de toutes les couches de convolution\n",
        "for name, param in model.named_parameters():\n",
        "    if 'layer1' in name:\n",
        "        param.requires_grad = False\n",
        "\n",
        "# Remplacer la dernière couche entièrement connectée pour la classification par une nouvelle couche adaptée à notre nombre de classes\n",
        "num_classes = len(train_data_norm.classes)\n",
        "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "\n",
        "# Définir la fonction de perte et l'optimiseur\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Entraînement du modèle\n",
        "start_time = time.time()\n",
        "for epoch in range(5):  # 5 epochs pour l'exemple\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader_norm:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "    epoch_loss = running_loss / len(train_data_norm)\n",
        "    print(f\"Epoch {epoch+1}/{5} - Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Training Time: {end_time - start_time} seconds\")\n",
        "train_accuracy = calculate_accuracy(train_loader_norm, model)\n",
        "print(train_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5V6FPXEUMeFr"
      },
      "source": [
        "le modèle pré-entraîné, mais en laissant tous les paramètres (incluant les couches de convolution) se faire ajuster par backprop) deuxieme normalisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2fLuRUUgBC6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77d73dc0-42ac-4635-8b60-1a7527c01fb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 - Loss: 3.5141\n",
            "Epoch 2/5 - Loss: 1.9131\n",
            "Epoch 3/5 - Loss: 1.3390\n",
            "Epoch 4/5 - Loss: 0.9762\n",
            "Epoch 5/5 - Loss: 0.7238\n",
            "Training Time: 351.7113380432129 seconds\n",
            "80.93991807009559\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Charger le modèle pré-entraîné ResNet18\n",
        "model = resnet18(pretrained=True)\n",
        "\n",
        "# Remplacer la dernière couche entièrement connectée pour la classification par une nouvelle couche adaptée à notre nombre de classes\n",
        "num_classes = len(train_data.classes)\n",
        "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "\n",
        "# Définir la fonction de perte et l'optimiseur\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Entraînement du modèle\n",
        "start_time = time.time()\n",
        "for epoch in range(5):  # 5 epochs pour l'exemple\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "    epoch_loss = running_loss / len(train_data)\n",
        "    print(f\"Epoch {epoch+1}/{5} - Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Training Time: {end_time - start_time} seconds\")\n",
        "train_accuracy = calculate_accuracy(train_loader, model)\n",
        "print(train_accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkgcJutfNVVR"
      },
      "source": [
        "le modèle pré-entraîné, mais en laissant tous les paramètres (incluant les couches de convolution) se faire ajuster par backprop) premiere normalisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Es6IuTZqfrF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26baec6c-1e2f-4384-daf3-1d4f1894cdfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 - Loss: 3.5205\n",
            "Epoch 2/5 - Loss: 1.9295\n",
            "Epoch 3/5 - Loss: 1.3172\n",
            "Epoch 4/5 - Loss: 1.0222\n",
            "Epoch 5/5 - Loss: 0.7380\n",
            "Training Time: 352.5062870979309 seconds\n",
            "72.84934000910333\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Charger le modèle pré-entraîné ResNet18\n",
        "model = resnet18(pretrained=True)\n",
        "\n",
        "# Remplacer la dernière couche entièrement connectée pour la classification par une nouvelle couche adaptée à notre nombre de classes\n",
        "num_classes = len(train_data.classes)\n",
        "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "\n",
        "# Définir la fonction de perte et l'optimiseur\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Entraînement du modèle\n",
        "start_time = time.time()\n",
        "for epoch in range(5):  # 5 epochs pour l'exemple\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader_norm:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "    epoch_loss = running_loss / len(train_data_norm)\n",
        "    print(f\"Epoch {epoch+1}/{5} - Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Training Time: {end_time - start_time} seconds\")\n",
        "train_accuracy = calculate_accuracy(train_loader_norm, model)\n",
        "print(train_accuracy)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}